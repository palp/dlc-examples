{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04b5736-0e0e-4b5d-a83d-2b5265489d76",
   "metadata": {},
   "source": [
    "# Deploying Stable Diffusion using Stability AI DLC on AWS SageMaker\n",
    "\n",
    "## Example: Stable Diffusion XL v0.9 on PyTorch 2.0.1\n",
    "\n",
    "This example will deploy an endpoint running Stable Diffusion XL on AWS SageMaker using the Stability AI DLC. This example can provide inference as-is or serve as a basis for custom development & deployment scenarios.\n",
    "\n",
    "If you are looking for a turnkey solution for inference with a full-featured API, check out [SDXL on AWS Marketplace](https://aws.amazon.com/marketplace/seller-profile?id=seller-mybtdwpr2puau) and the related [Jumpstart notebooks](https://github.com/Stability-AI/aws-jumpstart-examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6c8d5-23e0-49e4-b554-28cbc671a743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: You may have to restart your kernel after installing boto3\n",
    "!pip install \"sagemaker>=2.173.0\" \"huggingface_hub>=0.16.4\" \"boto3>=1.28.9\" --upgrade --quiet\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import ModelPackage, get_execution_role\n",
    "\n",
    "from PIL import Image\n",
    "from typing import Union\n",
    "import io\n",
    "import os\n",
    "import base64\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149960eb-61a6-45d9-9a5b-7ba78cf28be4",
   "metadata": {},
   "source": [
    "## 1. Download the model weights\n",
    "\n",
    "### SDXL 0.9 is a non-commercial model. You must apply for access to download the [base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9) and [refiner model](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9) then add your [HuggingFace token](https://huggingface.co/settings/tokens) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51641deb-4fe6-46d5-85d0-473f9ef2aa37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['HUGGING_FACE_HUB_TOKEN'] = 'YOUR TOKEN HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46bffc4-2794-4b28-ad33-92d736e425bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "local_dir = './model'\n",
    "snapshot_download(\n",
    "    repo_id=\"stabilityai/stable-diffusion-xl-base-0.9\",    \n",
    "    allow_patterns=\"sd_xl_base_0.9.safetensors\",\n",
    "    local_dir=local_dir,\n",
    "    local_dir_use_symlinks=False)\n",
    "snapshot_download(\n",
    "    repo_id=\"stabilityai/stable-diffusion-xl-refiner-0.9\",\n",
    "    allow_patterns=\"sd_xl_refiner_0.9.safetensors\",    \n",
    "    local_dir=local_dir,\n",
    "    local_dir_use_symlinks=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10edf97d-2751-41a7-9d2f-dd8cba8f2c1f",
   "metadata": {},
   "source": [
    "## 2. Custom Inference Script Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c38662e-1459-49c0-a0d1-ca84e48c7336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p model/code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22537ff5-c48b-40e4-9bb2-f5585407e478",
   "metadata": {},
   "source": [
    "### Inference Script: Text2Image, Image2Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c695fe0-9192-46da-84be-8d33b8d94e33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile model/code/sdxl_inference.py\n",
    "from io import BytesIO\n",
    "from einops import rearrange\n",
    "import json\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pytorch_lightning import seed_everything\n",
    "import numpy as np\n",
    "from sagemaker_inference.errors import BaseInferenceToolkitError\n",
    "from sgm.inference.helpers import (\n",
    "    load_model_from_config,\n",
    "    get_input_image_tensor,\n",
    "    get_sampler,\n",
    "    get_discretization,\n",
    "    get_guider,\n",
    "    do_sample,\n",
    "    do_img2img,\n",
    "    apply_refiner,\n",
    "    embed_watermark,\n",
    "    Img2ImgDiscretizationWrapper,\n",
    ")\n",
    "import os\n",
    "\n",
    "\n",
    "def model_fn(model_dir, context=None):\n",
    "    # Enable the refiner by default\n",
    "    disable_refiner = os.environ.get(\"SDXL_DISABLE_REFINER\", \"false\").lower() == \"true\"\n",
    "\n",
    "    # This hardcoded path is needed due to an import error in the sgm library\n",
    "    sgm_path = \"/opt/ml/stability-ai/generative-models\"  # os.path.dirname(sgm.__file__)\n",
    "    base_config_path = os.path.join(sgm_path, \"configs/inference/sd_xl_base.yaml\")\n",
    "    refiner_config_path = os.path.join(sgm_path, \"configs/inference/sd_xl_refiner.yaml\")\n",
    "    base_model_path = os.path.join(model_dir, \"sd_xl_base_0.9.safetensors\")\n",
    "    refiner_model_path = os.path.join(model_dir, \"sd_xl_refiner_0.9.safetensors\")\n",
    "    print(f\"Loading base model config from {base_config_path}\")\n",
    "    base_config = OmegaConf.load(os.path.join(sgm_path, base_config_path))\n",
    "    print(f\"Loading base model from {base_model_path}\")\n",
    "    base_model, _ = load_model_from_config(base_config, base_model_path)\n",
    "    base_model.conditioner.half()\n",
    "    base_model.model.half()\n",
    "    if disable_refiner:\n",
    "        print(\"Refiner model disabled by SDXL_DISABLE_REFINER environment variable\")\n",
    "        refiner_model = None\n",
    "    else:\n",
    "        print(f\"Loading refiner model config from {refiner_config_path}\")\n",
    "        refiner_config = OmegaConf.load(os.path.join(sgm_path, refiner_config_path))\n",
    "        print(f\"Loading refiner model from {refiner_model_path}\")\n",
    "        refiner_model, _ = load_model_from_config(refiner_config, refiner_model_path)\n",
    "        refiner_model.conditioner.half()\n",
    "        refiner_model.model.half()\n",
    "\n",
    "    return {\"base\": base_model, \"refiner\": refiner_model}\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    if request_content_type == \"application/json\":\n",
    "        model_input = json.loads(request_body)\n",
    "        if not \"text_prompts\" in model_input:\n",
    "            raise BaseInferenceToolkitError(400, \"text_prompts missing\")\n",
    "        return model_input\n",
    "    else:\n",
    "        raise BaseInferenceToolkitError(400, \"Content-type must be application/json\")\n",
    "\n",
    "\n",
    "def predict_fn(data, model, context=None):\n",
    "    # Only a single positive and optionally a single negative prompt are supported by this example.\n",
    "    prompts = []\n",
    "    negative_prompts = []\n",
    "    if \"text_prompts\" in data:\n",
    "        for text_prompt in data[\"text_prompts\"]:\n",
    "            if \"text\" not in text_prompt:\n",
    "                raise BaseInferenceToolkitError(400, \"text missing from text_prompt\")\n",
    "            if \"weight\" not in text_prompt:\n",
    "                text_prompt[\"weight\"] = 1.0\n",
    "            if text_prompt[\"weight\"] < 0:\n",
    "                negative_prompts.append(text_prompt[\"text\"])\n",
    "            else:\n",
    "                prompts.append(text_prompt[\"text\"])\n",
    "\n",
    "    if len(prompts) != 1:\n",
    "        raise BaseInferenceToolkitError(\n",
    "            400, \"One prompt with positive or default weight must be supplied\"\n",
    "        )\n",
    "    if len(negative_prompts) > 1:\n",
    "        raise BaseInferenceToolkitError(\n",
    "            400, \"Only one negative weighted prompt can be supplied\"\n",
    "        )\n",
    "\n",
    "    seed = 0\n",
    "    height = 1024\n",
    "    width = 1024\n",
    "    sampler_name = \"DPMPP2MSampler\"\n",
    "    cfg_scale = 7.0\n",
    "    steps = 50\n",
    "    use_pipeline = model[\"refiner\"] is not None\n",
    "\n",
    "    if \"height\" in data:\n",
    "        height = data[\"height\"]\n",
    "    if \"width\" in data:\n",
    "        width = data[\"width\"]\n",
    "    if \"sampler\" in data:\n",
    "        sampler_name = data[\"sampler\"]\n",
    "    if \"cfg_scale\" in data:\n",
    "        cfg_scale = data[\"cfg_scale\"]\n",
    "    if \"steps\" in data:\n",
    "        steps = data[\"steps\"]\n",
    "    if \"seed\" in data:\n",
    "        seed = data[\"seed\"]\n",
    "        seed_everything(seed)\n",
    "    if \"use_pipeline\" in data:\n",
    "        use_pipeline = data[\"use_pipeline\"]\n",
    "\n",
    "    if model[\"refiner\"] is None and use_pipeline:\n",
    "        raise BaseInferenceToolkitError(400, \"Pipeline is not available\")\n",
    "\n",
    "    value_dict = {\n",
    "        \"prompt\": prompts[0],\n",
    "        \"negative_prompt\": negative_prompts[0] if len(negative_prompts) > 0 else \"\",\n",
    "        \"aesthetic_score\": 6.0,\n",
    "        \"negative_aesthetic_score\": 2.5,\n",
    "        \"orig_height\": height,\n",
    "        \"orig_width\": width,\n",
    "        \"target_height\": height,\n",
    "        \"target_width\": width,\n",
    "        \"crop_coords_top\": 0,\n",
    "        \"crop_coords_left\": 0,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        sampler = get_sampler(\n",
    "            sampler_name=sampler_name,\n",
    "            steps=steps,\n",
    "            discretization_config=get_discretization(\"LegacyDDPMDiscretization\"),\n",
    "            guider_config=get_guider(guider=\"VanillaCFG\", scale=cfg_scale),\n",
    "        )\n",
    "\n",
    "        output = do_sample(\n",
    "            model=model[\"base\"],\n",
    "            sampler=sampler,\n",
    "            value_dict=value_dict,\n",
    "            num_samples=1,\n",
    "            H=height,\n",
    "            W=width,\n",
    "            C=4,\n",
    "            F=8,\n",
    "            return_latents=use_pipeline,\n",
    "        )\n",
    "\n",
    "        if isinstance(output, (tuple, list)):\n",
    "            samples, samples_z = output\n",
    "        else:\n",
    "            samples = output\n",
    "            samples_z = None\n",
    "\n",
    "        if use_pipeline and samples_z is not None:\n",
    "            print(\"Running Refinement Stage\")\n",
    "            refiner_sampler = get_sampler(\n",
    "                sampler_name=\"EulerEDMSampler\",\n",
    "                steps=50,\n",
    "                discretization_config=get_discretization(\"LegacyDDPMDiscretization\"),\n",
    "                guider_config=get_guider(guider=\"VanillaCFG\", scale=5.0),\n",
    "            )\n",
    "            refiner_sampler.discretization = Img2ImgDiscretizationWrapper(sampler.discretization, strength=0.3)\n",
    "\n",
    "            samples = apply_refiner(\n",
    "                input=samples_z,\n",
    "                model=model[\"refiner\"],\n",
    "                sampler=refiner_sampler,\n",
    "                num_samples=1,\n",
    "                prompt=prompts[0],\n",
    "                negative_prompt=negative_prompts[0]\n",
    "                if len(negative_prompts) > 0\n",
    "                else \"\",\n",
    "            )\n",
    "\n",
    "        samples = embed_watermark(samples)\n",
    "        images = []\n",
    "        for sample in samples:\n",
    "            sample = 255.0 * rearrange(sample.cpu().numpy(), \"c h w -> h w c\")\n",
    "            image_bytes = BytesIO()\n",
    "            Image.fromarray(sample.astype(np.uint8)).save(image_bytes, format=\"PNG\")\n",
    "            image_bytes.seek(0)\n",
    "            images.append(image_bytes.read())\n",
    "\n",
    "        return images\n",
    "\n",
    "    except ValueError as e:\n",
    "        raise BaseInferenceToolkitError(400, str(e))\n",
    "\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    # This only returns a single image since that's all the example code supports\n",
    "    if accept != \"image/png\":\n",
    "        raise BaseInferenceToolkitError(400, \"Accept header must be image/png\")\n",
    "    return prediction[0], accept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb15579-a52b-4d5f-8bc6-76698e0295ac",
   "metadata": {},
   "source": [
    "## 3. Package and upload model archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd3d986-6b7f-4308-a2cf-983541040d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3d436a-9c5e-4992-8d65-a529477d4a15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rerun this cell only if you need to re-upload the weights, otherwise you can reuse the existing model_package_name and upload only your new code \n",
    "from sagemaker.utils import name_from_base\n",
    "model_package_name = name_from_base(f\"sdxl-v0-9\") # You may want to make this a fixed name of your choosing instead\n",
    "model_uri = f's3://{sagemaker_session_bucket}/{model_package_name}/'\n",
    "print(f'Uploading base model to {model_uri}, this will take a while...')\n",
    "!aws s3 cp model/sd_xl_base_0.9.safetensors {model_uri}\n",
    "print(f'Uploading refiner model to {model_uri}, this will take a while...')\n",
    "!aws s3 cp model/sd_xl_refiner_0.9.safetensors {model_uri}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0455a3b9-96d0-4fc1-a0b0-f7508c90018e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rerun this cell when you have changed the code or are uploading a fresh copy of the weights\n",
    "print(f'Uploading code to {model_uri}code')\n",
    "!aws s3 cp model/code/sdxl_inference.py {model_uri}code/sdxl_inference.py\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd6072-9ef6-4a32-b93a-87f7067a1183",
   "metadata": {},
   "source": [
    "## 4. Create and deploy a model and perform real-time inference\n",
    "\n",
    "boto3 is being used to deploy the model here to take advantage of [Uncompressed model downloads](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-uncompressed.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd680a-7af0-4060-ba62-25341dd58973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_image_uri = '740929234339.dkr.ecr.us-east-1.amazonaws.com/stabilityai-pytorch-inference:2.0.1-sgm0.0.1-gpu-py310-cu118-ubuntu20.04-sagemaker-2023-07-24-03-47-13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673db27-8c8e-4815-b6b3-afff123dac63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = name_from_base(f\"sdxl-v0-9\")\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "create_model_response = sagemaker_client.create_model(\n",
    "    ModelName = endpoint_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = {\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataSource\": {\n",
    "            \"S3DataSource\": {               # S3 Data Source configuration:\n",
    "                \"S3Uri\": model_uri,         # path to your model and script\n",
    "                \"S3DataType\": \"S3Prefix\",   # causes SageMaker to download from a prefix\n",
    "                \"CompressionType\": \"None\"   # disables compression\n",
    "            }\n",
    "        },\n",
    "        \"Environment\": {\n",
    "            \"SAGEMAKER_PROGRAM\": \"sdxl_inference.py\",  # This script was uploaded to the data source above             \n",
    "            \"TS_DEFAULT_RESPONSE_TIMEOUT\": \"1000\", # We need a long timeout for the model to load\n",
    "            \"HUGGINGFACE_HUB_CACHE\": \"/tmp/cache/huggingface/hub\" # Put this cache somewhere with a lot of space\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "create_endpoint_config_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_name,\n",
    "    ProductionVariants = [{\n",
    "        \"ModelName\": endpoint_name,\n",
    "        \"VariantName\": \"sdxl\",\n",
    "        \"InitialInstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.g5.4xlarge\",     # 4xlarge is required to load the model\n",
    "    }]\n",
    ")\n",
    "        \n",
    "\n",
    "deploy_model_response = sagemaker_client.create_endpoint(\n",
    "    EndpointName = endpoint_name,\n",
    "    EndpointConfigName = endpoint_name\n",
    ")\n",
    "    \n",
    "print('Waiting for the endpoint to be in service, this can take 5-10 minutes...')\n",
    "waiter = sagemaker_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "print(f'Endpoint {endpoint_name} is in service, but the model is still loading. This may take another 5-10 minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd21303-b63a-4ec1-ac2f-83cf79e22084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import BytesDeserializer\n",
    "\n",
    "# Create a predictor with proper serializers\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name, \n",
    "    sagemaker_session=sess,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=BytesDeserializer(accept=\"image/png\")\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9378b7-9bdc-41bf-a7b4-401c6de94b1c",
   "metadata": {},
   "source": [
    "## A. Text to image\n",
    "\n",
    "**Note**: The endpoint will be \"InService\" before the model has finished loading, so this request will initially time out. Check the endpoint logs in CloudWatch for status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8cdb1a-f6aa-4350-b241-ef1b240e789d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = predictor.predict({\"text_prompts\": [{\"text\":\"A photograph of fresh pizza with basil and tomatoes, from a traditional oven\", \"weight\": 1}],                                             \n",
    "                                             \"seed\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce12b64-ac31-45e1-bdd1-318445c71582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_and_show(model_response) -> None:\n",
    "    \"\"\"\n",
    "    Decodes and displays an image from SDXL output\n",
    "\n",
    "    Args:\n",
    "        model_response (GenerationResponse): The response object from the deployed SDXL model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"        \n",
    "    image = Image.open(io.BytesIO(model_response))\n",
    "    display(image)\n",
    "\n",
    "decode_and_show(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22d0135-1a61-4d4e-a555-10465fe40e35",
   "metadata": {},
   "source": [
    "## B. Image to image **NOT YET IMPLEMENTED IN THIS EXAMPLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5faa61-01ee-4bb4-ace7-22c402cc445d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_image(image_path: str, resize: bool = True) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    Encode an image as a base64 string, optionally resizing it to 512x512.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the image file.\n",
    "        resize (bool, optional): Whether to resize the image. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Union[str, None]: The encoded image as a string, or None if encoding failed.\n",
    "    \"\"\"\n",
    "    assert os.path.exists(image_path)\n",
    "\n",
    "    if resize:\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((512, 512))\n",
    "        image_base = os.path.splitext(image_path)[0]\n",
    "        image_resized_path = f\"{image_base}_resized.png\"\n",
    "        image.save(image_resized_path)\n",
    "        image_path = image_resized_path\n",
    "    image = Image.open(image_path)\n",
    "    assert image.size == (512, 512)\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        img_byte_array = image_file.read()\n",
    "        # Encode the byte array as a Base64 string\n",
    "        try:\n",
    "            base64_str = base64.b64encode(img_byte_array).decode(\"utf-8\")\n",
    "            return base64_str\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to encode image {image_path} as base64 string.\")\n",
    "            print(e)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184979a1-9072-4e21-903d-ef7d689fe1ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! wget https://platform.stability.ai/Cat_August_2010-4.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30c1ff-3762-4ede-ad11-306070d741a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here is the original image:\n",
    "display(Image.open('Cat_August_2010-4.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986bba40-805b-42ab-a2c9-2eaf5810a0f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_path = \"Cat_August_2010-4.jpg\"\n",
    "cat_data = encode_image(cat_path)\n",
    "\n",
    "output = deployed_model.predict(GenerationRequest(text_prompts=[TextPrompt(text=\"cat in watercolour\")],\n",
    "                                                  init_image= cat_data,\n",
    "                                                  cfg_scale=9,\n",
    "                                                  image_strength=0.8,\n",
    "                                                  seed=42\n",
    "                                                  ))\n",
    "decode_and_show(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcff5ec-fbe8-4891-9477-31bf8938fc79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = deployed_model.predict(GenerationRequest(text_prompts=[TextPrompt(text=\"cat painted by Basquiat\")],\n",
    "                                                  init_image= cat_data,\n",
    "                                            cfg_scale=17,\n",
    "                                            image_strength=0.4,\n",
    "                                             seed=42\n",
    "                                             ))\n",
    "decode_and_show(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54f8af-0c13-4ef5-9b19-665a8db1befa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws sagemaker list-endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a79e2e-714a-464c-be54-96c60ade8fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete an endpoint\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "# Rerun the aws cli command above to confirm that its gone."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
